{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f2ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The theory is inspired from:\n",
    "# https://jaketae.github.io/study/word2vec/#forward-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1319e1",
   "metadata": {},
   "source": [
    "### Word2Vec as defined by Wikipedia:\n",
    "\n",
    "<b>Word2vec is a technique for natural language processing published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7030bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0344a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text as an input\n",
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948cf8e",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "\n",
    "<b>A simple method to convert sentences or paragraphs into their granular entity format i.e. words<br>\n",
    "<i>Example string : \"The bottle is on the table\"</i><br>\n",
    "\n",
    "The string gets converted to python list after tokenization.<br>\n",
    "\n",
    "<i>Tokenized string : [\"The\", \"bottle\", \"is\", \"on\", \"the\", \"table\"]</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3d1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unwanted special characters\n",
    "text = re.sub(\",\", \"\", text)\n",
    "\n",
    "# Simple tokenization function returns \n",
    "def tokenize(text):\n",
    "    return re.findall(r'[A-Za-z0-9]+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99dd253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for the input string : \n",
      " {'mathematical', 'through', 'study', 'a', 'difficult', 'tasks', 'in', 'the', 'as', 'develop', 'seen', 'conventional', 'of', 'applications', 'so', 'data', 'make', 'without', 'computer', 'decisions', 'it', 'being', 'infeasible', 'wide', 'programmed', 'variety', 'is', 'improve', 'build', 'predictions', 'filtering', 'sample', 'known', 'artificial', 'automatically', 'model', 'training', 'or', 'order', 'are', 'based', 'email', 'that', 'subset', 'on', 'machine', 'such', 'do', 'where', 'needed', 'to', 'learning', 'algorithms', 'experience', 'intelligence', 'explicitly', 'used', 'and', 'vision', 'perform'}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "print(\"Tokens for the input string : \\n\", set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394854c",
   "metadata": {},
   "source": [
    "<b>As computers cannot process string data, we need a way to convert our string token into numerical format. One such operation is to provide indices to our tokens. therefore we create a map between tokens and indices, and vice versa.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7fedf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mathematical': 0, 'through': 1, 'study': 2, 'a': 3, 'difficult': 4, 'tasks': 5, 'in': 6, 'the': 7, 'as': 8, 'develop': 9, 'seen': 10, 'conventional': 11, 'of': 12, 'applications': 13, 'so': 14, 'data': 15, 'make': 16, 'without': 17, 'computer': 18, 'decisions': 19, 'it': 20, 'being': 21, 'infeasible': 22, 'wide': 23, 'programmed': 24, 'variety': 25, 'is': 26, 'improve': 27, 'build': 28, 'predictions': 29, 'filtering': 30, 'sample': 31, 'known': 32, 'artificial': 33, 'automatically': 34, 'model': 35, 'training': 36, 'or': 37, 'order': 38, 'are': 39, 'based': 40, 'email': 41, 'that': 42, 'subset': 43, 'on': 44, 'machine': 45, 'such': 46, 'do': 47, 'where': 48, 'needed': 49, 'to': 50, 'learning': 51, 'algorithms': 52, 'experience': 53, 'intelligence': 54, 'explicitly': 55, 'used': 56, 'and': 57, 'vision': 58, 'perform': 59}\n"
     ]
    }
   ],
   "source": [
    "# the function returns proper mapping for tokens\n",
    "def mapping(tokens):\n",
    "    id_2_word = {i:j for (i,j) in enumerate(set(tokens))}\n",
    "    word_2_id = {j:i for (i,j) in id_2_word.items()}\n",
    "    \n",
    "    return word_2_id, id_2_word\n",
    "\n",
    "word_2_id, id_2_word = mapping(tokens)\n",
    "\n",
    "# the below dictionary contains the key as words \n",
    "# and value as its respecive indices.\n",
    "# the reverse mapping of the same can be used a lookup table,\n",
    "# to find out the word given its indices\n",
    "print(word_2_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ff732",
   "metadata": {},
   "source": [
    "### Generating Training Data\n",
    "\n",
    "<b>Word2Vec is a form of supervised learning technique, but we are only given with a paragraph to start with so what could be our features and targets.<br> \n",
    "\n",
    "we loop through each word/token in the sentence/corpus. In each loop we look at words at the left and right of the input word(also called as center-word) within a given window called as context words. The left and right words will be paired with the center word to create pairs of words and that will be used as our training data.</b>\n",
    "\n",
    "<b>Example sentence : <i>\"A crazy fox <u>jumped</u> over the cow.\"</i><br>\n",
    "For the above sentence, lets consider our window to be 2 and our center-word to be <i>\"jumped\"</i>.</b>\n",
    "\n",
    "<b>Therefore, \n",
    "our left context words are: <i>[\"crazy\", \"fox\"]</i><br>\n",
    "our right context words are: <i>[\"over\", \"the\"]</i><br>\n",
    "\n",
    "Our training pairs will come out as:<i><br>\n",
    "[\"jumped\", \"crazy\"], <br>\n",
    "[\"jumped\", \"fox\"], <br>\n",
    "[\"jumped\", \"over\"], <br>\n",
    "[\"jumped\", \"the\"] <br></i>\n",
    "\n",
    "Each word whether it is input word or context word will be represented using One-hot vectors, and our final goal is to create equivalent word embeddings which can show the semantic relationship among the words from corpus.</b>\n",
    "\n",
    "<b>Below is the code blocks that generates training data using the algorithm described above. we basically iterate over the input paragraph and generate pairs</b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a863bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2021)\n",
    "        \n",
    "def one_hot_encode(id, vocab_size):\n",
    "    \"\"\"\n",
    "    return one hot encoded vector for the words\n",
    "    \"\"\"\n",
    "    res = [0] * vocab_size\n",
    "    res[id] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b1d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(word, word_idx, window, sentence):\n",
    "    #print(word_idx)\n",
    "    # length of the input sentence\n",
    "    n_len = len(sentence.strip().split(\" \"))\n",
    "    # left word indexes\n",
    "    left_words = range(max(0, word_idx - window), word_idx)\n",
    "    # right word indexes\n",
    "    right_words = range(word_idx, min(n_len, word_idx + window+1))\n",
    "    \n",
    "    # consolidating left and right context word indexes\n",
    "    context_words_idx = list(left_words) + list(right_words)\n",
    "    try:\n",
    "        context_words_idx.remove(word_idx)\n",
    "    except:\n",
    "        print(\"excepted  word : \", word_idx)\n",
    "        \n",
    "    # collecting all the context words\n",
    "    context_words = [sentence.strip().split(\" \")[i] for i in context_words_idx]\n",
    "    yield from context_words\n",
    "\n",
    "def get_training_data(text, window = 2):\n",
    "    # empty lists for context and center word\n",
    "    X, y = [], []\n",
    "    for sentence in text.lower().split(\".\"):\n",
    "        # stripping of unwanted spaces\n",
    "        sentence = sentence.strip()\n",
    "        for idx, word in enumerate(sentence.split(\" \")):\n",
    "            center_word = word\n",
    "            context_words = get_pair(center_word, idx, window, sentence)\n",
    "        \n",
    "            for c_word in context_words:\n",
    "                X.append(one_hot_encode(word_2_id[c_word], len(word_2_id)))\n",
    "                y.append(one_hot_encode(word_2_id[center_word],len(word_2_id)))\n",
    "    \n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e8b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data(text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e769c880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : (312, 60), y : (312, 60)\n"
     ]
    }
   ],
   "source": [
    "# checking the dimensionality of data \n",
    "# to get a sense of what matrices we are working with.\n",
    "print(\"X : {}, y : {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fc841",
   "metadata": {},
   "source": [
    "<b>Both <i>X</i> and <i>y</i> are matrices with 312 rows and 60 columns. Here, 312 is the number of training examples we have. The number of rows would have been larger if we would have used a larger window. 60 is the size of our corpus, since we have one hot encoded both input and output as 60-dimensional sparse vectors, this is expected.</b>\n",
    "\n",
    "<b>Now, we are ready to build and train our embedding network</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0150f",
   "metadata": {},
   "source": [
    "### The Embedding Model\n",
    "\n",
    "<b>The expected outcome from the word2vec is to generate an embedding for input words which when used with input words will result in the prediction of context words. These embeddings are nothing but rows of the intermediate weight matrices which we get after training our model with intermediate hidden layers and a softmax output layer</b>\n",
    "\n",
    "<b>For example:</b> \n",
    "<b>lets take an example of 3 samples from the corpus denoted as X<sub>3</sub>. X<sub>3</sub> has a shape of (3,60), we take a weight matrix W<sub>1</sub> with the shape (60,2). The dot product of X<sup>T</sup><sub>3</sub> with W<sub>1</sub> results in a matrix with shape of (3,2). The resultant matrix of shape (3,2) is exactly what we want to achieve with embedding: representing words as dense vectors, an increment from simple one hot encoding.</b>\n",
    "\n",
    "<b>The <a href = \"https://jaketae.github.io/study/word2vec/#forward-propagation\">post</a> by Jaesung Tae describes the whole idea of embedding in a much more illustrated and detailed manner.</b>\n",
    "\n",
    "<b>Moving on, we have our data ready for training, but to make sure that our model trains in a good way we would be comparing our results from softmax output that to true one-hot encoded targets. The error function we use with softmax is cross entropy, defined as:</b>\n",
    "<br><br>\n",
    "<center><b><i>$H(p,q) = - \\sum p(x).log(q(x))$</i></b></center>\n",
    "\n",
    "<b>The vectors we are dealing with are one-hot encoded therefore if I sum the sum the values of a vector it will always results in 1.</b>\n",
    "\n",
    "<b>Therefore, we can rewrite the loss function equation as:</b>\n",
    "<br><br>\n",
    "<center><b><i>$H(p,q) = - p.log(q)$</i></b></center>\n",
    "\n",
    "<b>Where <i>p</i> is the true labels and <i>q</i> is the predicted one.</b>\n",
    "\n",
    "<b>Now, The whole process can be represented in a set of equation which will be used to calculated values in Forward Propagration as well.</b>\n",
    "<br><br>\n",
    "<center><i>$X = input(matrix)$</i></center>\n",
    "<center><i>$A_{1} = X.W_{1}$</i></center>\n",
    "<center><i>$A_{2} = A_{1}.W_{2}$</i></center>\n",
    "<center><i>$Z = Softmax(A_{2})$</i></center>\n",
    "\n",
    "<b>Where $X$ is input, $W_{1}$ and $W_{2}$ are weight matrices and $Z$ is the matrix contains the prediction probability vectors.</b>\n",
    "\n",
    "### Code Implementation\n",
    "\n",
    "<b>In the below code we started off with our input size as number of tokens in our training set, In the very first layer the inputs will be converted into the shape of the embedding. On the next layers we are again converting the embedded matrix back into number of tokens, that is because we want our results/embedded matrix to be in similar shape as that of true labels.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "541f6f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(vocab_size, n_embedding):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    vocab_size : number of tokens in our training set\n",
    "    n_embedding : the size of embedding we want\n",
    "    \"\"\"\n",
    "    model = {\n",
    "        \"W1\" : np.random.randn(vocab_size, n_embedding),\n",
    "        \"W2\" : np.random.randn(n_embedding, vocab_size)\n",
    "    }\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6420ee",
   "metadata": {},
   "source": [
    "<b>Let's specify our model to create 20-dimensional embeddings. In other words each token now will be converted from 60-dimensions to 20-dimensions. The actual word2vec model used much higher dimensions such as 300, but for our purposes this is not necessary.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ac8a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 1.48860905,  0.67601087, -0.41845137, ..., -0.22508127,\n",
       "         -1.33620597,  0.30372151],\n",
       "        [-0.72015884,  2.5449146 ,  1.31729112, ..., -0.16150753,\n",
       "         -2.79353057,  0.36164801],\n",
       "        [ 0.24010758,  0.47781228,  0.20885194, ..., -0.67335092,\n",
       "          1.22222171, -0.92641342],\n",
       "        ...,\n",
       "        [ 0.5095208 , -0.35721995, -0.9493845 , ..., -0.15412878,\n",
       "          0.66883833, -1.34445507],\n",
       "        [-0.28392442, -1.69575813, -0.37903759, ...,  0.0637477 ,\n",
       "         -1.54333106,  0.76194403],\n",
       "        [-0.05690875, -0.49331203,  0.50302151, ...,  0.25543456,\n",
       "         -1.60934907,  0.15491553]]),\n",
       " 'W2': array([[-2.14142059, -0.42644362,  3.28832288, ..., -1.24307108,\n",
       "         -1.18852196,  0.02933298],\n",
       "        [-0.44975585, -0.65242634,  1.48745667, ..., -1.15040527,\n",
       "          0.5292788 ,  1.26745163],\n",
       "        [ 0.33728638,  0.48471877,  0.38711267, ...,  0.48105195,\n",
       "          0.26199591,  0.45999949],\n",
       "        ...,\n",
       "        [-0.21879945, -0.7016483 ,  1.62768672, ...,  0.03223237,\n",
       "         -0.38291534, -0.13306572],\n",
       "        [ 0.08213361,  0.62819253, -0.84862612, ..., -1.06915126,\n",
       "          0.92771856, -1.85218944],\n",
       "        [-0.84187279, -1.15505875,  0.31037359, ...,  0.9316116 ,\n",
       "          0.42592694, -1.07324521]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = init_network(len(word_2_id), 20)\n",
    "\n",
    "# Current state representation of model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46288c",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "<b>Coding forward propagation is relatively easy as we just need to rewrite the equations from <i>\"The Embedding Model\"</i> section into code.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cbfed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, X, return_cache = True):\n",
    "    cache = {}\n",
    "    cache['A1'] = np.dot(X, model['W1'])\n",
    "    cache['A2'] = np.dot(cache['A1'], model['W2'])\n",
    "    cache['Z'] = softmax(cache['A2'])\n",
    "    \n",
    "    if not return_cache:\n",
    "        return cache['Z']\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72794588",
   "metadata": {},
   "source": [
    "<b>For backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called <i>cache</i>. However, if we simply want the final prediction vector only, not the intermediate variables, we set <i>return_cache</i> to <i>False</i>.</b>\n",
    "\n",
    "<b>For softmax function, we are expecting a matrix instead of a vector as an output from $A_{2}$, so we will need to make some changes accordingly.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed02acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    z_exp = np.exp(X)\n",
    "    return z_exp / np.sum(z_exp, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc35b43",
   "metadata": {},
   "source": [
    "### Checking the dimensionality\n",
    "\n",
    "<b>Before moving on we want to test our model and parameters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "439ffa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 20)\n",
      "(312, 60)\n",
      "(312, 60)\n"
     ]
    }
   ],
   "source": [
    "example_a1 = np.dot(X, model['W1'])\n",
    "print(example_a1.shape)\n",
    "example_a2 = np.dot(example_a1, model['W2'])\n",
    "print(example_a2.shape)\n",
    "example_z = softmax(example_a2)\n",
    "print(example_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc3aa4",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "<b>In backpropagation we try to find out our derivatives of loss function with respect to our parameters, currently in our case $W_{1}$ and $W_{2}$</b>\n",
    "\n",
    "<b>Fortunately, we have already calculated the equations which we require for backpropagation in this <a href = \"https://github.com/Pi-Akash/Pytorch-Learnings/blob/main/Neural%20Network%20from%20Scratch.ipynb\">notebook</a> where we implement neural network from scratch, Calculation of backpropagation is an important step and there are many good resources available in the internet, therefore I am not going to describe how we got to these equations, if you want to learn more about backpropagation, please go through the below links:</b>\n",
    "\n",
    "- <a href = \"https://deepnotes.io/softmax-crossentropy\">How to calculate derivative of Cross Entropy Loss when used with Softmax?</a>\n",
    "- <a href = \"http://colah.github.io/posts/2015-08-Backprop/\">Calculus on Computational Graphs: Backpropagation</a>\n",
    "- <a href = \"https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\">Yes you should understand backprop by Andrej Karpathy</a>\n",
    "\n",
    "<b>Ok, lets look at the equations:</b><br>\n",
    "$del_{3} = Z - y$<br>\n",
    "$del_{2} = del_{3} * Softmax(A_{2}) * (1 - Softmax(A_{2}))$<br>\n",
    "$del_{Loss}/del_{w2} = A_{1}^{T} . del_{2}$<br>\n",
    "$del_{Loss}/del_{w1} = X^{T} . (del_{2} . W_{2}^{T})$\n",
    "\n",
    "<b>In the below code block we perform a small sanity check to make sure our dimensions are proper as per the above equations</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d54d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del3 :  (312, 60)\n",
      "del2 :  (312, 60)\n",
      "delW2 :  (20, 60)\n",
      "delW1 :  (60, 20)\n"
     ]
    }
   ],
   "source": [
    "cache = forward(model, X)\n",
    "del3 = cache['Z'] - y\n",
    "print(\"del3 : \", del3.shape)\n",
    "del2 = del3 * softmax(cache['A2']) * (1 - softmax(cache['A2']))\n",
    "print(\"del2 : \", del2.shape)\n",
    "delW2 = np.dot(cache['A1'].T, del2)\n",
    "print(\"delW2 : \", delW2.shape)\n",
    "delW1 = np.dot(X.T, np.dot(del2, model['W2'].T))\n",
    "print(\"delW1 : \", delW1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bb688",
   "metadata": {},
   "source": [
    "<b>Now that the dimensions are proper, Lets implement the backpropagation function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f397e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, X, y, learning_rate = 0.01):\n",
    "    cache = forward(model, X)\n",
    "    del3 = cache['Z'] - y\n",
    "    del2 = del3 * softmax(cache['A2']) * (1 - softmax(cache['A2']))\n",
    "    delW2 = np.dot(cache['A1'].T, del2)\n",
    "    delW1 = np.dot(X.T, np.dot(del2, model['W2'].T))\n",
    "    \n",
    "    assert delW2.shape == model['W2'].shape, \"delW2 is not the same shape as W2\"\n",
    "    assert delW1.shape == model['W1'].shape, \"delW1 is not the same shape as W1\"\n",
    "    model['W1'] = model['W1'] - learning_rate * delW1\n",
    "    model['W2'] = model['W2'] - learning_rate * delW2\n",
    "    \n",
    "    return cross_entropy(cache['Z'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4629e5",
   "metadata": {},
   "source": [
    "<b>As specified above, we will be using log-loss or Cross entropy loss to keep record of the error between our predictions and true labels, the cross entropy loss can be easily implemented as follows.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e223028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(z, y):\n",
    "    return -1 * np.sum(np.log(z) * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97f4d4",
   "metadata": {},
   "source": [
    "<b>We have created all the components required to train a neural network, Just for assurance we are going to list down all the functions we have created.</b>\n",
    "\n",
    "- <i>init_network(vocab_size, n_embedding) : Initialization of model parameters</i>\n",
    "- <i>forward(model, X, return_cache = True) : Forward propagation</i>\n",
    "- <i>softmax(X) : applies softmax on the input value</i>\n",
    "- <i>backward(model, X, y, learning_rate = 0.01) : Backward propagation</i>\n",
    "- <i>cross_entropy(z, y) : calculates log loss</i>\n",
    "\n",
    "<b>Using above functions we will test our model on the training set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37db7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000 # number of iterations to run\n",
    "learning_rate = 0.05 # learning rate of the model\n",
    "model = init_network(len(word_2_id), 20) # initialization of model\n",
    "\n",
    "# looping forward and backward pass for n iterations:\n",
    "history = [backward(model, X, y, learning_rate) for _ in range(n_iter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34e0d4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(history)), history)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Loss v/s Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663ce4b",
   "metadata": {},
   "source": [
    "<b>Looking at the above plot, it seems we did well. The loss starting at a value of 3000+ comes down to less than 1000 after 1000 iterations. we can fiddle with learning rate and number of iterations to get much more fine tuned results to reduce the loss even further. With some degree of confidence we can say that the embedding layer has been trained well.</b>\n",
    "\n",
    "<b>Lets test our trained model on a word and see what context words we get for the input word.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bd3c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random word is :  algorithms\n",
      "learning\n",
      "machine\n",
      "computer\n",
      "of\n",
      "improve\n",
      "used\n",
      "build\n",
      "that\n",
      "develop\n",
      "conventional\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2021)\n",
    "random_word_idx = np.random.randint(len(word_2_id))\n",
    "print(\"Random word is : \", id_2_word[random_word_idx])\n",
    "\n",
    "random_word = one_hot_encode(random_word_idx, len(word_2_id))\n",
    "result = forward(model, [random_word], return_cache = False)\n",
    "result = np.squeeze(result)\n",
    "\n",
    "# printing top 10 words\n",
    "for word in (id_2_word[id] for id in np.argsort(result)[::-1][:10]):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe02a8c",
   "metadata": {},
   "source": [
    "<b>Input text for reference:</b>\n",
    "\n",
    "<i>Machine learning is the study <b>of computer</b> algorithms <b>that improve</b> automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. <b>Machine learning</b> algorithms <b>are used</b> in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.</i>\n",
    "\n",
    "<b>We can see that the bold words in the paragraphs are the context words for our input word and out of top 10 predicted words first 4-5 are same as that of highlighted ones. This concludes that our model words as expected.</b>\n",
    "\n",
    "<b>Now lets look at the embeddings that got created for the training dataset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d54777bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape :  (60, 20)\n",
      "\n",
      "[[ 0.03316042 -1.70377684 -0.69516723 ...  0.04795336  0.89781764\n",
      "  -0.45716584]\n",
      " [-0.60903402 -0.50675367 -0.44897499 ...  0.0967588   0.86857157\n",
      "   0.76617485]\n",
      " [-2.58565507 -1.23073185  2.17199634 ...  0.35676115  1.52885652\n",
      "  -0.9592579 ]\n",
      " ...\n",
      " [ 0.94002151 -1.20604718  0.14270202 ...  0.89383132  0.74153647\n",
      "  -0.14949186]\n",
      " [-0.35239205  0.35581766 -0.7969699  ... -0.0517016  -0.10718259\n",
      "   0.03535121]\n",
      " [-0.52581913  0.3420522   1.48293876 ...  0.2204532   1.18039095\n",
      "  -1.65384786]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding shape : \", model['W1'].shape)\n",
    "print()\n",
    "print(model['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef38677",
   "metadata": {},
   "source": [
    "<b>Lets make embeddings a little more user friendly for display.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6eb713e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, word):\n",
    "    try:\n",
    "        idx = word_2_id[word]\n",
    "    except:\n",
    "        print(\"Word not in corpus\")\n",
    "    one_hot = one_hot_encode(idx, len(word_2_id))\n",
    "    return forward(model, [one_hot])['A1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3b24b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random word is :  algorithms\n",
      "Embedding shape :  (1, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.59280032,  0.46352374,  0.42955392, -0.79421328,  0.55034104,\n",
       "         1.0170684 , -0.07856808, -0.16202304,  0.40609382,  0.38084355,\n",
       "        -1.27358747, -1.06305941,  0.87758973, -1.21262052,  0.53966545,\n",
       "         0.05435573,  0.50984065,  0.30313373, -0.69539455,  0.25823635]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2021)\n",
    "random_word_idx = np.random.randint(len(word_2_id))\n",
    "print(\"Random word is : \", id_2_word[random_word_idx])\n",
    "print(\"Embedding shape : \", get_embedding(model, id_2_word[random_word_idx]).shape)\n",
    "get_embedding(model, id_2_word[random_word_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee476f",
   "metadata": {},
   "source": [
    "<b>Above we tested a random word from the corpus and got a 20 dimensional vector as expected.</b>\n",
    "\n",
    "<b>And of course, thisvector is not a collection of some randomly initialized numbers, but a result of training with context data generated through the sliding window algorithm described above.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
